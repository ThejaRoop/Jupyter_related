{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3348c825-3b5e-4004-a59f-7c7afefb5c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Data Report:\n",
      " {'name': 1, 'email': 2}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    \"id\": [1, 2, 3, 4, 5],\n",
    "    \"name\": [\"Alice\", \"Bob\", None, \"David\", \"Eve\"],\n",
    "    \"email\": [\"alice@example.com\", None, \"charlie@example.com\", \"david@example.com\", None]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Check for missing values\n",
    "# missing_values = df.isnull().sum()\n",
    "# print(\"Missing Values:\\n\", missing_values)\n",
    "\n",
    "# Generate a report for missing data\n",
    "def generate_missing_report(df):\n",
    "    report = {}\n",
    "    for column in df.columns:\n",
    "        if df[column].isnull().sum() > 0:\n",
    "            report[column] = df[column].isnull().sum()\n",
    "    return report\n",
    "\n",
    "missing_report = generate_missing_report(df)\n",
    "print(\"Missing Data Report:\\n\", missing_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cc45da-49e7-4e31-bfb9-497301df5e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Lineage Tracking\n",
    "source = DataSource(\"Customer Data\")\n",
    "source.add_lineage(\"Extracted from CRM\")\n",
    "source.add_lineage(\"Cleansed and transformed\")\n",
    "source.add_lineage(\"Loaded into Data Warehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff98bbfc-028c-4b7b-90dd-3f719b732708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGovernance:\n",
    "    def __init__(self):\n",
    "        self.permissions = {\n",
    "            \"admin\": [\"read\", \"write\", \"delete\"],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75bab3d-1243-46c6-a408-87f200b9cd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Infrastructure as Code (IaC) with Boto3: Provisioning AWS Resources - automate the creation of AWS resources.\n",
    "\n",
    "#Automating Monitoring with CloudWatch - CloudWatch alarms for metrics like CPU usage.\n",
    "\n",
    "#Database Connection Handling: Connect to PostgreSQL with psycopg2 - connect applications to databases, manage transactions, and run queries.\n",
    "\n",
    "#Configuration Management: Using Pydantic for Configuration Validation**  -- validate and manage configurations for services and deployments.\n",
    "\n",
    "#Service Orchestration: Creating RESTful APIs with FastAPI -- develop APIs to integrate services within their infrastructure\n",
    "\n",
    "--\n",
    "# Deploying Docker Containers on Kubernetes with Python Client**\n",
    "# Data Transformation Pipeline: Using PySpark for Big Data Processing\n",
    "#Automated Certificate Renewal: Using ACME Protocol for SSL Certificates - SSL certificate renewal, often for web servers\n",
    "#, IT architects might automate CI/CD pipelines. Here’s a Python example that triggers a Jenkins job using the Jenkins REST API.\n",
    "#architects need to dynamically update load balancers, here’s an example of modifying an Nginx configuration to add servers.\n",
    "\n",
    "# 19. Webhook Integration for Monitoring\n",
    "# Architects integrate monitoring tools with incident management systems to automate alerts. \n",
    "# Here’s a script that sends an HTTP POST request to a webhook if a server health check fails.\n",
    "\n",
    "# 18. Data Archiving to Reduce Costs on AWS S3\n",
    "# This script moves old data from an S3 bucket to Glacier storage to reduce costs, a common data lifecycle management task.\n",
    "\n",
    "\n",
    "# 17. Automated Backup for RDS Databases\n",
    "# Architects may automate database backups to ensure data redundancy and restore capabilities.\n",
    "\n",
    "\n",
    "# 16. Dynamic Scaling of Cloud Resources Based on Load\n",
    "# This script scales EC2 instances based on average CPU utilization, a common task for architects managing cloud infrastructure.\n",
    "\n",
    "\n",
    "# 15. Server Health Check and Auto-Healing\n",
    "# Architects often need to ensure server uptime by periodically checking server health. \n",
    "# This script pings a list of servers and restarts any unresponsive ones.\n",
    "\n",
    "# 14. Data Masking for Privacy Compliance\n",
    "# To comply with regulations like GDPR, data architects may mask sensitive data in production environments. \n",
    "# The example below replaces sensitive fields with asterisks, useful in data anonymization before sharing logs with external teams.\n",
    "\n",
    "\n",
    "# 13. Automated Incident Response with AWS Lambda and SNS\n",
    "# Architects may set up automated responses to incidents, such as shutting down a compromised EC2 instance.\n",
    "# The following Lambda function checks an EC2 instance's \n",
    "# tags and shuts it down if flagged for termination. This function could be triggered by an SNS topic that alerts on security issues.\n",
    "\n",
    "\n",
    "# 12. Multi-Region Failover Script for High Availability\n",
    "# To ensure high availability, IT architects set up multi-region failover. \n",
    "# In the event of a failure, the script below switches DNS records to an alternative region using AWS Route 53.\n",
    "\n",
    "\n",
    "# 11. Message Queue Monitoring: Check Kafka Topic Lag**\n",
    "# For streaming architectures, architects often monitor Kafka for potential lag in topics.\n",
    "\n",
    "# 10. Load Balancing: Programmatically Add Servers to an Nginx Configuration**\n",
    "# In cases where architects need to dynamically update load balancers, here’s an example of modifying an Nginx configuration to add servers.\n",
    "\n",
    "\n",
    "# 9. CI/CD Pipeline Automation: Triggering Jenkins Jobs Programmatically**\n",
    "# In DevOps workflows, IT architects might automate CI/CD pipelines. Here’s a Python example that triggers a Jenkins job using the Jenkins REST API.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13479a5e-5d1d-4b52-a178-f73f4dc36d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def create_s3_bucket(bucket_name, region='us-east-1'):\n",
    "    \"\"\"Creates an S3 bucket in the specified AWS region.\"\"\"\n",
    "    try:\n",
    "        if region == 'us-east-1':\n",
    "            s3.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            s3.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "        print(f'Bucket {bucket_name} created successfully.')\n",
    "    except Exception as e:\n",
    "        print(f'Error creating bucket: {e}')\n",
    "\n",
    "# Example usage\n",
    "create_s3_bucket('my-architects-demo-bucket', 'us-west-2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a9c74f-e499-4984-b9b8-943078e90144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "\n",
    "def create_cpu_alarm(instance_id, threshold=80):\n",
    "    \"\"\"Creates a CloudWatch alarm for high CPU utilization.\"\"\"\n",
    "    try:\n",
    "        cloudwatch.put_metric_alarm(\n",
    "            AlarmName=f'HighCPUAlarm-{instance_id}',\n",
    "            MetricName='CPUUtilization',\n",
    "            Namespace='AWS/EC2',\n",
    "            Statistic='Average',\n",
    "            Period=300,\n",
    "            EvaluationPeriods=2,\n",
    "            Threshold=threshold,\n",
    "            ComparisonOperator='GreaterThanThreshold',\n",
    "            AlarmActions=['arn:aws:sns:us-east-1:123456789012:MyTopic'],  # Replace with actual ARN\n",
    "            Dimensions=[{'Name': 'InstanceId', 'Value': instance_id}]\n",
    "        )\n",
    "        print(f'Alarm created for instance {instance_id}.')\n",
    "    except Exception as e:\n",
    "        print(f'Error creating alarm: {e}')\n",
    "\n",
    "# Example usage\n",
    "create_cpu_alarm('i-1234567890abcdef0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17981a29-7c77-4909-a773-f73c98f7ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "def connect_to_db(host, dbname, user, password):\n",
    "    \"\"\"Establishes a connection to a PostgreSQL database.\"\"\"\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            host=host,\n",
    "            database=dbname,\n",
    "            user=user,\n",
    "            password=password\n",
    "        )\n",
    "        print(\"Connected to the database.\")\n",
    "        return connection\n",
    "    except Exception as e:\n",
    "        print(f'Error connecting to the database: {e}')\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "conn = connect_to_db('localhost', 'mydatabase', 'myuser', 'mypassword')\n",
    "\n",
    "# Perform queries and transactions\n",
    "if conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    print(cursor.fetchone())\n",
    "    cursor.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a558f8c-7940-48e9-b796-c140f35c62f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseSettings, Field\n",
    "\n",
    "class AppConfig(BaseSettings):\n",
    "    db_host: str = Field(..., env=\"DB_HOST\")\n",
    "    db_port: int = Field(5432, env=\"DB_PORT\")\n",
    "    db_name: str = Field(..., env=\"DB_NAME\")\n",
    "    db_user: str = Field(..., env=\"DB_USER\")\n",
    "    db_password: str = Field(..., env=\"DB_PASSWORD\")\n",
    "    log_level: str = Field('INFO', env=\"LOG_LEVEL\")\n",
    "\n",
    "    class Config:\n",
    "        env_file = \".env\"\n",
    "\n",
    "# Load configuration\n",
    "config = AppConfig()\n",
    "\n",
    "print(\"Database Host:\", config.db_host)\n",
    "print(\"Log Level:\", config.log_level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b590967-120c-41e5-b305-0c7d80ec4565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Item(BaseModel):\n",
    "    name: str\n",
    "    description: str = None\n",
    "    price: float\n",
    "    quantity: int\n",
    "\n",
    "inventory = {}\n",
    "\n",
    "@app.post(\"/items/{item_id}\")\n",
    "async def create_item(item_id: int, item: Item):\n",
    "    if item_id in inventory:\n",
    "        raise HTTPException(status_code=400, detail=\"Item already exists\")\n",
    "    inventory[item_id] = item\n",
    "    return item\n",
    "\n",
    "@app.get(\"/items/{item_id}\")\n",
    "async def read_item(item_id: int):\n",
    "    if item_id not in inventory:\n",
    "        raise HTTPException(status_code=404, detail=\"Item not found\")\n",
    "    return inventory[item_id]\n",
    "\n",
    "# Run with `uvicorn script_name:app --reload`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7bc0398-f6b5-41ca-9984-07e4d1d7d2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if duplicate return true\n",
    "c=[1,3,4,4,2]\n",
    "class containdup:\n",
    "    def havingdupl(c):\n",
    "        c1=[]\n",
    "        for x in c:\n",
    "            if x not in c1:\n",
    "                c1.append(x)\n",
    "            return True\n",
    "        return False\n",
    "f=containdup.havingdupl(c)\n",
    "f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "559974ff-7378-48f0-8363-0cdc4d3440bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = [3,4,5,6]\n",
    "target = 7\n",
    "\n",
    "\n",
    "def twosum(nums,target):\n",
    "    records={}\n",
    "    for index, val in enumerate(nums):\n",
    "        if target-val in records:\n",
    "            return [index,records[target-val]]\n",
    "        else:\n",
    "            records[val]=index\n",
    "\n",
    "twosum(nums,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6791594c-f3cd-46cc-91d6-0757a8ebeafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fwefestyn\n"
     ]
    }
   ],
   "source": [
    "#palindrome check\n",
    "c='nytsefewf'\n",
    "# for x in range(len(c)-1,-1,-1):\n",
    "#     print(c[x])\n",
    "c1=c[::-1]\n",
    "print(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283311b7-2b63-411b-91f6-a7342cf1b771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
